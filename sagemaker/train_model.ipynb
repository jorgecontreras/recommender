{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import Session\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=\"us-east-2\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "\n",
    "# Define AWS resources\n",
    "s3_bucket = \"recommender-system-jcontreras\"\n",
    "s3_prefix = \"training-data\"\n",
    "sagemaker_role = \"arn:aws:iam::<YOUR__ACCOUNT_ID>:role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Initialize boto3 clients\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    # Load dataset\n",
    "    movies_file = f\"s3://{s3_bucket}/movies.csv\"\n",
    "    ratings_file = f\"s3://{s3_bucket}/ratings.csv\"\n",
    "\n",
    "    # Read the data\n",
    "    movies_df = pd.read_csv(movies_file)\n",
    "    ratings_df = pd.read_csv(ratings_file)\n",
    "\n",
    "    # Print column information for debugging\n",
    "    print(\"Movies columns:\", movies_df.columns)\n",
    "    print(\"Ratings columns:\", ratings_df.columns)\n",
    "\n",
    "    # Merge datasets\n",
    "    data = pd.merge(ratings_df, movies_df, on=\"movieId\")\n",
    "\n",
    "    # Prepare data for training\n",
    "    data = data[[\"userId\", \"movieId\", \"rating\"]]\n",
    "\n",
    "    # Encode user_id and movie_id as integers\n",
    "    data[\"userId\"] = data[\"userId\"].astype(\"category\").cat.codes\n",
    "    data[\"movieId\"] = data[\"movieId\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # Extract the number of features\n",
    "    feature_dim = 2  # userId and movieId are the features\n",
    "    print(\"Feature Dimension:\", feature_dim)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Separate features (X) and labels (y)\n",
    "    train_X = train[[\"userId\", \"movieId\"]].values.astype(\"float32\")\n",
    "    train_y = train[\"rating\"].values.astype(\"float32\")\n",
    "    test_X = test[[\"userId\", \"movieId\"]].values.astype(\"float32\")\n",
    "    test_y = test[\"rating\"].values.astype(\"float32\")\n",
    "\n",
    "    # Convert to RecordIO protobuf format\n",
    "    def write_recordio(data, labels, file_path):\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            smac.write_numpy_to_dense_tensor(f, data, labels)\n",
    "\n",
    "    write_recordio(train_X, train_y, \"train.recordio\")\n",
    "    write_recordio(test_X, test_y, \"test.recordio\")\n",
    "\n",
    "    # Upload RecordIO files to S3\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_client.upload_file(\"train.recordio\", s3_bucket, f\"{s3_prefix}/train/train.recordio\")\n",
    "    s3_client.upload_file(\"test.recordio\", s3_bucket, f\"{s3_prefix}/test/test.recordio\")\n",
    "\n",
    "    print(\"Data preprocessing completed and uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Step 2: Train model\n",
    "def train_model():\n",
    "    session = Session()\n",
    "    container = get_image_uri(boto3.Session().region_name, \"factorization-machines\")\n",
    "\n",
    "    # Define estimator\n",
    "    estimator = Estimator(\n",
    "        container,\n",
    "        role=sagemaker_role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        output_path=f\"s3://{s3_bucket}/output\"\n",
    "    )\n",
    "    \n",
    "    estimator.set_hyperparameters(\n",
    "        feature_dim=2,\n",
    "        num_factors=64,\n",
    "        predictor_type=\"binary_classifier\"\n",
    "    )\n",
    "\n",
    "    # Specify input data\n",
    "    train_input = sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{s3_bucket}/{s3_prefix}/train/\",\n",
    "        content_type=\"application/x-recordio-protobuf\"\n",
    "    )\n",
    "\n",
    "    validation_input = sagemaker.inputs.TrainingInput(\n",
    "        s3_data=f\"s3://{s3_bucket}/{s3_prefix}/test/\",\n",
    "        content_type=\"application/x-recordio-protobuf\"\n",
    "    )\n",
    "\n",
    "    estimator.fit({\n",
    "        \"train\": train_input,\n",
    "        \"validation\": validation_input\n",
    "    })\n",
    "\n",
    "    print(\"Model training completed.\")\n",
    "    \n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Deploy model\n",
    "def deploy_model(estimator):\n",
    "    # Deploy the model\n",
    "    predictor = estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        endpoint_name=\"recommender-endpoint\"\n",
    "    )\n",
    "\n",
    "    print(f\"Model deployed at endpoint: recommender-endpoint\")\n",
    "    return predictor\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_preprocess_data()\n",
    "    trained_estimator = train_model()\n",
    "    deploy_model(trained_estimator)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
